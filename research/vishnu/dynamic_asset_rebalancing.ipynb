{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Define the Portfolio Environment\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data: np.ndarray, initial_cash: float = 10000.0):\n",
    "        \"\"\"\n",
    "        Initialize the portfolio environment.\n",
    "        :param data: Historical asset price data (numpy array).\n",
    "        :param initial_cash: Starting capital.\n",
    "        \"\"\"\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.initial_cash = initial_cash\n",
    "        self.num_assets = data.shape[1]\n",
    "        \n",
    "        # Define state space and action space\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.num_assets + 2,))\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.num_assets,))\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.portfolio = np.zeros(self.num_assets)\n",
    "        logger.info(f\"Environment reset. Initial cash: {self.cash}, Portfolio: {self.portfolio}\")\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        :param action: Allocation proportions for each asset.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Step {self.current_step}: Taking action: {action}\")\n",
    "        \n",
    "        # Normalize action to ensure it sums to 1\n",
    "        action = action / np.sum(action)\n",
    "        \n",
    "        # Calculate portfolio rebalancing\n",
    "        current_prices = self.data[self.current_step]\n",
    "        portfolio_value = np.dot(self.portfolio, current_prices) + self.cash\n",
    "        new_portfolio = portfolio_value * action / current_prices\n",
    "        transaction_costs = np.sum(np.abs(new_portfolio - self.portfolio)) * 0.001  # 0.1% fee\n",
    "        reward = portfolio_value - transaction_costs - self.cash\n",
    "        \n",
    "        # Update state\n",
    "        self.cash = portfolio_value - np.sum(new_portfolio * current_prices)\n",
    "        self.portfolio = new_portfolio\n",
    "        logger.info(f\"New portfolio: {self.portfolio}, Remaining cash: {self.cash}, Reward: {reward}\")\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Generate the current observation.\n",
    "        \"\"\"\n",
    "        current_prices = self.data[self.current_step]\n",
    "        portfolio_value = np.dot(self.portfolio, current_prices)\n",
    "        return np.concatenate([current_prices, [portfolio_value, self.cash]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Environment reset. Initial cash: 10000.0, Portfolio: [0. 0. 0. 0. 0.]\n",
      "Environment reset. Initial cash: 10000.0, Portfolio: [0. 0. 0. 0. 0.]\n",
      "Step 0: Taking action: [0.688157   0.26861453 0.6727024  0.09545094 0.02135996]\n",
      "New portfolio: [45.06016206 10.60309212 31.26794487  4.9751072   1.86452892], Remaining cash: -0.0005029141902923584, Reward: -0.0937708351702895\n",
      "Step 1: Taking action: [0.12593919 0.9422795  0.10287945 0.21460946 0.31195104]\n",
      "New portfolio: [ 9.71692433 85.45719202  3.8114488   9.86471044 13.06957754], Remaining cash: 0.0001280379274248844, Reward: 8592.326575664405\n",
      "Step 2: Taking action: [0.5267901 0.7105647 0.370498  0.9189529 0.8909605]\n",
      "New portfolio: [44.91172803 21.4548904  12.34101065 57.25584655 57.99612226], Remaining cash: -0.0005650876846630126, Reward: 15168.756841828805\n",
      "Step 3: Taking action: [0.33872604 0.27471083 0.5177382  0.3314631  0.7941609 ]\n",
      "New portfolio: [35.10495131 24.19282305 35.78386179 25.1908329  71.08924047], Remaining cash: -0.001071823153324658, Reward: 15984.104816685238\n",
      "Step 4: Taking action: [0.6592678  0.25653347 0.9188288  0.805649   0.17602627]\n",
      "New portfolio: [36.34750327 24.59051918 71.10336166 57.0041644  11.28620251], Remaining cash: 0.0010290107311448082, Reward: 17263.807785069996\n",
      "Step 5: Taking action: [0.97109574 0.08142666 0.2474573  0.17061533 0.16617629]\n",
      "New portfolio: [94.39479294 14.53850047 30.47968827 19.51098408 37.98978857], Remaining cash: 0.00022851671747048385, Reward: 20447.155759711848\n",
      "Step 6: Taking action: [0.54956    0.846894   0.6831356  0.9696475  0.47194603]\n",
      "New portfolio: [30.36290743 77.28659967 73.97898901 40.95145837 19.7041149 ], Remaining cash: 0.0, Reward: 21546.361232180658\n",
      "Step 7: Taking action: [0.59927297 0.6896334  0.64169014 0.01576268 0.46166155]\n",
      "New portfolio: [40.54222288 75.86708122 95.03529124  1.17819062 43.46582626], Remaining cash: -0.0004863500507781282, Reward: 21314.77689308906\n",
      "Step 8: Taking action: [0.4973898  0.4272184  0.76838356 0.38598895 0.671267  ]\n",
      "New portfolio: [54.12083224 29.0559001  97.32078485 18.53744185 59.87755799], Remaining cash: 0.0005547618602577131, Reward: 18614.623142873046\n",
      "End of the episode reached.\n"
     ]
    }
   ],
   "source": [
    "# Simulating Sample Data\n",
    "np.random.seed(42)\n",
    "sample_data = np.random.uniform(low=50, high=150, size=(10, 5))  # 5 assets, 10 timesteps\n",
    "\n",
    "# Running the environment\n",
    "env = PortfolioEnv(data=sample_data, initial_cash=10000.0)\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(len(sample_data)):\n",
    "    # Sample a random action (proportions for each asset)\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        logger.info(\"End of the episode reached.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
